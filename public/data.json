[{"method":"add","arguments":["s0","0"],"result":"s0","user_bottom_stack":"latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_meta_registrations.py, line 2183 in _formula>"},{"method":"sub","arguments":["s0","1"],"result":"s0 - 1","user_bottom_stack":"latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_meta_registrations.py, line 2183 in _formula>"},{"method":"sub","arguments":["s0 - 1","1"],"result":"s0 - 2","user_bottom_stack":"latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_meta_registrations.py, line 2183 in _formula>"},{"method":"int_floordiv","arguments":["s0 - 2","2"],"result":"((s0//2)) - 1","user_bottom_stack":"latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_meta_registrations.py, line 2183 in _formula>"},{"method":"add","arguments":["((s0//2)) - 1","1"],"result":"(s0//2)","user_bottom_stack":"latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_meta_registrations.py, line 2183 in _formula>"},{"method":"add","arguments":["s1","0"],"result":"s1","user_bottom_stack":"latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_meta_registrations.py, line 2183 in _formula>"},{"method":"sub","arguments":["s1","1"],"result":"s1 - 1","user_bottom_stack":"latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_meta_registrations.py, line 2183 in _formula>"},{"method":"sub","arguments":["s1 - 1","1"],"result":"s1 - 2","user_bottom_stack":"latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_meta_registrations.py, line 2183 in _formula>"},{"method":"int_floordiv","arguments":["s1 - 2","2"],"result":"((s1//2)) - 1","user_bottom_stack":"latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_meta_registrations.py, line 2183 in _formula>"},{"method":"add","arguments":["((s1//2)) - 1","1"],"result":"(s1//2)","user_bottom_stack":"latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_meta_registrations.py, line 2183 in _formula>"},{"method":"gt","arguments":["(s0//2)","0"],"result":"((s0//2)) > 0","user_bottom_stack":"latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_meta_registrations.py, line 2256 in <genexpr>>"},{"method":"lt","arguments":["(s1//2)","1"],"result":"((s1//2)) < 1","user_bottom_stack":"latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_refs/__init__.py, line 4800 in new_empty>"},{"method":"lt","arguments":["(s0//2)","1"],"result":"((s0//2)) < 1","user_bottom_stack":"latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_refs/__init__.py, line 4800 in new_empty>"},{"method":"eq","arguments":["s0*s1","0"],"result":"False","user_bottom_stack":"latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1883 in are_strides_like_channels_last>"},{"method":"lt","arguments":["s0*s1","0"],"result":"False","user_bottom_stack":"latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},{"method":"gt","arguments":["s0*s1","1"],"result":"s0*s1 > 1","user_bottom_stack":"latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1895 in are_strides_like_channels_last>"},{"method":"mul","arguments":["s0*s1","16"],"result":"16*s0*s1","user_bottom_stack":"latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},{"method":"eq","arguments":["s1","0"],"result":"False","user_bottom_stack":"latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},{"method":"gt","arguments":["16*s0*s1","1"],"result":"True","user_bottom_stack":"latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},{"method":"eq","arguments":["((s0//2))*((s1//2))","-1"],"result":"Eq(((s0//2))*((s1//2)), -1)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 931 in infer_size>"},{"method":"ge","arguments":["((s0//2))*((s1//2))","0"],"result":"((s0//2))*((s1//2)) >= 0","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 934 in infer_size>"},{"method":"mul","arguments":["((s0//2))*((s1//2))","3072"],"result":"3072*((s0//2))*((s1//2))","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 935 in infer_size>"},{"method":"eq","arguments":["3072*((s0//2))*((s1//2))","3072*((s0//2))*((s1//2))"],"result":"True","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 940 in infer_size>"},{"method":"eq","arguments":["3072*((s0//2))*((s1//2))","0"],"result":"Eq(3072*((s0//2))*((s1//2)), 0)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_refs/__init__.py, line 3716 in _reshape_view_helper>"},{"method":"eq","arguments":["((s0//2))*((s1//2))","(s0//2)"],"result":"Eq(((s0//2))*((s1//2)), (s0//2))","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_refs/__init__.py, line 3784 in _reshape_view_helper>"},{"method":"mod","arguments":["(s0//2)","((s0//2))*((s1//2))"],"result":"Mod((s0//2), ((s0//2))*((s1//2)))","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_refs/__init__.py, line 3793 in _reshape_view_helper>"},{"method":"ne","arguments":["Mod((s0//2), ((s0//2))*((s1//2)))","0"],"result":"Ne(Mod((s0//2), ((s0//2))*((s1//2))), 0)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_refs/__init__.py, line 3793 in _reshape_view_helper>"},{"method":"mul","arguments":["(s0//2)","(s1//2)"],"result":"((s0//2))*((s1//2))","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_refs/__init__.py, line 3795 in _reshape_view_helper>"},{"method":"mod","arguments":["((s0//2))*((s1//2))","((s0//2))*((s1//2))"],"result":"0","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_refs/__init__.py, line 3793 in _reshape_view_helper>"},{"method":"ne","arguments":["0","0"],"result":"False","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_refs/__init__.py, line 3793 in _reshape_view_helper>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1398 in _collapse_view_helper>"},{"method":"eq","arguments":["(s1//2)","0"],"result":"Eq((s1//2), 0)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1399 in _collapse_view_helper>"},{"method":"eq","arguments":["(s0//2)","1"],"result":"Eq((s0//2), 1)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1405 in _collapse_view_helper>"},{"method":"mul","arguments":["(s1//2)","(s0//2)"],"result":"((s0//2))*((s1//2))","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1408 in _collapse_view_helper>"},{"method":"gt","arguments":["(s1//2)","1"],"result":"((s1//2)) > 1","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1409 in _collapse_view_helper>"},{"method":"gt","arguments":["3072*((s0//2))*((s1//2))","0"],"result":"3072*((s0//2))*((s1//2)) > 0","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1415 in _collapse_view_helper>"},{"method":"ne","arguments":["(s1//2)","1"],"result":"Ne((s1//2), 1)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1416 in _collapse_view_helper>"},{"method":"mul","arguments":["(s1//2)","1"],"result":"(s1//2)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},{"method":"eq","arguments":["(s1//2)","(s1//2)"],"result":"True","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},{"method":"eq","arguments":["3072*((s0//2))*((s1//2))","0"],"result":"Eq(3072*((s0//2))*((s1//2)), 0)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1427 in _collapse_view_helper>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1398 in _collapse_view_helper>"},{"method":"eq","arguments":["(s1//2)","0"],"result":"Eq((s1//2), 0)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1399 in _collapse_view_helper>"},{"method":"eq","arguments":["(s0//2)","1"],"result":"Eq((s0//2), 1)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1405 in _collapse_view_helper>"},{"method":"mul","arguments":["(s1//2)","(s0//2)"],"result":"((s0//2))*((s1//2))","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1408 in _collapse_view_helper>"},{"method":"gt","arguments":["(s1//2)","1"],"result":"((s1//2)) > 1","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1409 in _collapse_view_helper>"},{"method":"gt","arguments":["3072*((s0//2))*((s1//2))","0"],"result":"3072*((s0//2))*((s1//2)) > 0","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1415 in _collapse_view_helper>"},{"method":"ne","arguments":["(s1//2)","1"],"result":"Ne((s1//2), 1)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1416 in _collapse_view_helper>"},{"method":"mul","arguments":["(s1//2)","1"],"result":"(s1//2)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},{"method":"eq","arguments":["(s1//2)","(s1//2)"],"result":"True","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},{"method":"eq","arguments":["3072*((s0//2))*((s1//2))","0"],"result":"Eq(3072*((s0//2))*((s1//2)), 0)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1427 in _collapse_view_helper>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1398 in _collapse_view_helper>"},{"method":"eq","arguments":["(s1//2)","0"],"result":"Eq((s1//2), 0)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1399 in _collapse_view_helper>"},{"method":"eq","arguments":["(s0//2)","1"],"result":"Eq((s0//2), 1)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1405 in _collapse_view_helper>"},{"method":"mul","arguments":["(s1//2)","(s0//2)"],"result":"((s0//2))*((s1//2))","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1408 in _collapse_view_helper>"},{"method":"gt","arguments":["(s1//2)","1"],"result":"((s1//2)) > 1","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1409 in _collapse_view_helper>"},{"method":"gt","arguments":["3072*((s0//2))*((s1//2))","0"],"result":"3072*((s0//2))*((s1//2)) > 0","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1415 in _collapse_view_helper>"},{"method":"ne","arguments":["(s1//2)","1"],"result":"Ne((s1//2), 1)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1416 in _collapse_view_helper>"},{"method":"mul","arguments":["(s1//2)","1"],"result":"(s1//2)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},{"method":"eq","arguments":["(s1//2)","(s1//2)"],"result":"True","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},{"method":"eq","arguments":["3072*((s0//2))*((s1//2))","0"],"result":"Eq(3072*((s0//2))*((s1//2)), 0)","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1427 in _collapse_view_helper>"},{"method":"ne","arguments":["((s0//2))*((s1//2))","((s0//2))*((s1//2))"],"result":"False","user_bottom_stack":"latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_refs/__init__.py, line 3813 in _reshape_view_helper>"},{"method":"int_floordiv","arguments":["s0","2"],"result":"(s0//2)","user_bottom_stack":"height = height // self.patch_size  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:522 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_dynamo/variables/builder.py, line 2322 in _wrap_fx_proxy>"},{"method":"int_floordiv","arguments":["s1","2"],"result":"(s1//2)","user_bottom_stack":"width = width // self.patch_size  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:523 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_dynamo/variables/builder.py, line 2322 in _wrap_fx_proxy>"},{"method":"gt","arguments":["(s0//2)","192"],"result":"((s0//2)) > 192","user_bottom_stack":"if height > self.pos_embed_max_size:  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:524 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_dynamo/variables/tensor.py, line 1164 in create>"},{"method":"gt","arguments":["(s1//2)","192"],"result":"((s1//2)) > 192","user_bottom_stack":"if width > self.pos_embed_max_size:  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:528 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_dynamo/variables/tensor.py, line 1164 in create>"},{"method":"sub","arguments":["(s0//2)","192"],"result":"192 - ((s0//2))","user_bottom_stack":"top = (self.pos_embed_max_size - height) // 2  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:533 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_dynamo/variables/builder.py, line 2322 in _wrap_fx_proxy>"},{"method":"int_floordiv","arguments":["192 - ((s0//2))","2"],"result":"(((-((s0//2)))//2)) + 96","user_bottom_stack":"top = (self.pos_embed_max_size - height) // 2  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:533 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_dynamo/variables/builder.py, line 2322 in _wrap_fx_proxy>"},{"method":"sub","arguments":["(s1//2)","192"],"result":"192 - ((s1//2))","user_bottom_stack":"left = (self.pos_embed_max_size - width) // 2  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:534 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_dynamo/variables/builder.py, line 2322 in _wrap_fx_proxy>"},{"method":"int_floordiv","arguments":["192 - ((s1//2))","2"],"result":"(((-((s1//2)))//2)) + 96","user_bottom_stack":"left = (self.pos_embed_max_size - width) // 2  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:534 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_dynamo/variables/builder.py, line 2322 in _wrap_fx_proxy>"},{"method":"add","arguments":["(((-((s0//2)))//2)) + 96","(s0//2)"],"result":"((s0//2)) + (((-((s0//2)))//2)) + 96","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_dynamo/variables/builder.py, line 2322 in _wrap_fx_proxy>"},{"method":"add","arguments":["(((-((s1//2)))//2)) + 96","(s1//2)"],"result":"((s1//2)) + (((-((s1//2)))//2)) + 96","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_dynamo/variables/builder.py, line 2322 in _wrap_fx_proxy>"},{"method":"lt","arguments":["(((-((s0//2)))//2)) + 96","0"],"result":"(((-((s0//2)))//2)) + 96 < 0","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 730 in slice_forward>"},{"method":"lt","arguments":["((s0//2)) + (((-((s0//2)))//2)) + 96","0"],"result":"((s0//2)) + (((-((s0//2)))//2)) + 96 < 0","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 733 in slice_forward>"},{"method":"lt","arguments":["(((-((s0//2)))//2)) + 96","0"],"result":"(((-((s0//2)))//2)) + 96 < 0","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 736 in slice_forward>"},{"method":"gt","arguments":["(((-((s0//2)))//2)) + 96","192"],"result":"(((-((s0//2)))//2)) + 96 > 192","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 738 in slice_forward>"},{"method":"lt","arguments":["((s0//2)) + (((-((s0//2)))//2)) + 96","(((-((s0//2)))//2)) + 96"],"result":"((s0//2)) + (((-((s0//2)))//2)) + 96 < (((-((s0//2)))//2)) + 96","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 741 in slice_forward>"},{"method":"eq","arguments":["((s0//2)) + (((-((s0//2)))//2)) + 96","9223372036854775807"],"result":"Eq(((s0//2)) + (((-((s0//2)))//2)) + 96, 9223372036854775807)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 743 in slice_forward>"},{"method":"gt","arguments":["((s0//2)) + (((-((s0//2)))//2)) + 96","192"],"result":"((s0//2)) + (((-((s0//2)))//2)) + 96 > 192","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 744 in slice_forward>"},{"method":"mul","arguments":["(((-((s0//2)))//2)) + 96","294912"],"result":"294912*(((-((s0//2)))//2)) + 28311552","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 748 in slice_forward>"},{"method":"add","arguments":["294912*(((-((s0//2)))//2)) + 28311552","0"],"result":"294912*(((-((s0//2)))//2)) + 28311552","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 748 in slice_forward>"},{"method":"sub","arguments":["((s0//2)) + (((-((s0//2)))//2)) + 96","(((-((s0//2)))//2)) + 96"],"result":"(s0//2)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 749 in slice_forward>"},{"method":"add","arguments":["(s0//2)","1"],"result":"((s0//2)) + 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 750 in slice_forward>"},{"method":"sub","arguments":["((s0//2)) + 1","1"],"result":"(s0//2)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 750 in slice_forward>"},{"method":"int_floordiv","arguments":["(s0//2)","1"],"result":"(s0//2)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 750 in slice_forward>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},{"method":"mul","arguments":["(s0//2)","294912"],"result":"294912*((s0//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},{"method":"gt","arguments":["294912*((s0//2))","1"],"result":"294912*((s0//2)) > 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},{"method":"mul","arguments":["(s0//2)","294912"],"result":"294912*((s0//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},{"method":"gt","arguments":["294912*((s0//2))","1"],"result":"294912*((s0//2)) > 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},{"method":"mul","arguments":["(s0//2)","294912"],"result":"294912*((s0//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},{"method":"gt","arguments":["294912*((s0//2))","1"],"result":"294912*((s0//2)) > 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},{"method":"lt","arguments":["(((-((s1//2)))//2)) + 96","0"],"result":"(((-((s1//2)))//2)) + 96 < 0","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 730 in slice_forward>"},{"method":"lt","arguments":["((s1//2)) + (((-((s1//2)))//2)) + 96","0"],"result":"((s1//2)) + (((-((s1//2)))//2)) + 96 < 0","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 733 in slice_forward>"},{"method":"lt","arguments":["(((-((s1//2)))//2)) + 96","0"],"result":"(((-((s1//2)))//2)) + 96 < 0","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 736 in slice_forward>"},{"method":"gt","arguments":["(((-((s1//2)))//2)) + 96","192"],"result":"(((-((s1//2)))//2)) + 96 > 192","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 738 in slice_forward>"},{"method":"lt","arguments":["((s1//2)) + (((-((s1//2)))//2)) + 96","(((-((s1//2)))//2)) + 96"],"result":"((s1//2)) + (((-((s1//2)))//2)) + 96 < (((-((s1//2)))//2)) + 96","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 741 in slice_forward>"},{"method":"eq","arguments":["((s1//2)) + (((-((s1//2)))//2)) + 96","9223372036854775807"],"result":"Eq(((s1//2)) + (((-((s1//2)))//2)) + 96, 9223372036854775807)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 743 in slice_forward>"},{"method":"gt","arguments":["((s1//2)) + (((-((s1//2)))//2)) + 96","192"],"result":"((s1//2)) + (((-((s1//2)))//2)) + 96 > 192","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 744 in slice_forward>"},{"method":"mul","arguments":["(((-((s1//2)))//2)) + 96","1536"],"result":"1536*(((-((s1//2)))//2)) + 147456","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 748 in slice_forward>"},{"method":"add","arguments":["294912*(((-((s0//2)))//2)) + 28311552","1536*(((-((s1//2)))//2)) + 147456"],"result":"294912*(((-((s0//2)))//2)) + 1536*(((-((s1//2)))//2)) + 28459008","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 748 in slice_forward>"},{"method":"sub","arguments":["((s1//2)) + (((-((s1//2)))//2)) + 96","(((-((s1//2)))//2)) + 96"],"result":"(s1//2)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 749 in slice_forward>"},{"method":"add","arguments":["(s1//2)","1"],"result":"((s1//2)) + 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 750 in slice_forward>"},{"method":"sub","arguments":["((s1//2)) + 1","1"],"result":"(s1//2)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 750 in slice_forward>"},{"method":"int_floordiv","arguments":["(s1//2)","1"],"result":"(s1//2)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 750 in slice_forward>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},{"method":"mul","arguments":["(s0//2)","294912"],"result":"294912*((s0//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},{"method":"gt","arguments":["294912*((s0//2))","1"],"result":"294912*((s0//2)) > 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},{"method":"mul","arguments":["(s0//2)","294912"],"result":"294912*((s0//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},{"method":"gt","arguments":["294912*((s0//2))","1"],"result":"294912*((s0//2)) > 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},{"method":"mul","arguments":["(s0//2)","294912"],"result":"294912*((s0//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},{"method":"gt","arguments":["294912*((s0//2))","1"],"result":"294912*((s0//2)) > 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},{"method":"mul","arguments":["(s0//2)","294912"],"result":"294912*((s0//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},{"method":"gt","arguments":["294912*((s0//2))","1"],"result":"294912*((s0//2)) > 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},{"method":"add","arguments":["294912*(((-((s0//2)))//2)) + 1536*(((-((s1//2)))//2)) + 28459008","0"],"result":"294912*(((-((s0//2)))//2)) + 1536*(((-((s1//2)))//2)) + 28459008","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 748 in slice_forward>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},{"method":"mul","arguments":["(s0//2)","294912"],"result":"294912*((s0//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},{"method":"gt","arguments":["294912*((s0//2))","1"],"result":"294912*((s0//2)) > 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},{"method":"mul","arguments":["(s0//2)","294912"],"result":"294912*((s0//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},{"method":"gt","arguments":["294912*((s0//2))","1"],"result":"294912*((s0//2)) > 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},{"method":"mul","arguments":["(s0//2)","294912"],"result":"294912*((s0//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},{"method":"gt","arguments":["294912*((s0//2))","1"],"result":"294912*((s0//2)) > 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},{"method":"mul","arguments":["(s0//2)","294912"],"result":"294912*((s0//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},{"method":"gt","arguments":["294912*((s0//2))","1"],"result":"294912*((s0//2)) > 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},{"method":"mul","arguments":["(s0//2)","294912"],"result":"294912*((s0//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},{"method":"gt","arguments":["294912*((s0//2))","1"],"result":"294912*((s0//2)) > 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},{"method":"lt","arguments":["(s1//2)","1"],"result":"((s1//2)) < 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_ops.py, line 758 in __call__>"},{"method":"lt","arguments":["(s0//2)","1"],"result":"((s0//2)) < 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_ops.py, line 758 in __call__>"},{"method":"eq","arguments":["1536*((s1//2))","0"],"result":"Eq(1536*((s1//2)), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1883 in are_strides_like_channels_last>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},{"method":"lt","arguments":["1536*((s1//2))","0"],"result":"1536*((s1//2)) < 0","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},{"method":"gt","arguments":["1536*((s1//2))","1"],"result":"1536*((s1//2)) > 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1895 in are_strides_like_channels_last>"},{"method":"mul","arguments":["1536*((s1//2))","(s0//2)"],"result":"1536*((s0//2))*((s1//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},{"method":"gt","arguments":["1536*((s0//2))*((s1//2))","1"],"result":"1536*((s0//2))*((s1//2)) > 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},{"method":"eq","arguments":["1536*((s1//2))","0"],"result":"Eq(1536*((s1//2)), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1883 in are_strides_like_channels_last>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},{"method":"lt","arguments":["1536*((s1//2))","0"],"result":"1536*((s1//2)) < 0","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},{"method":"gt","arguments":["1536*((s1//2))","1"],"result":"1536*((s1//2)) > 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1895 in are_strides_like_channels_last>"},{"method":"mul","arguments":["1536*((s1//2))","(s0//2)"],"result":"1536*((s0//2))*((s1//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},{"method":"gt","arguments":["1536*((s0//2))*((s1//2))","1"],"result":"1536*((s0//2))*((s1//2)) > 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},{"method":"eq","arguments":["1536*((s1//2))","0"],"result":"Eq(1536*((s1//2)), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1883 in are_strides_like_channels_last>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},{"method":"lt","arguments":["1536*((s1//2))","0"],"result":"1536*((s1//2)) < 0","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},{"method":"gt","arguments":["1536*((s1//2))","1"],"result":"1536*((s1//2)) > 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1895 in are_strides_like_channels_last>"},{"method":"mul","arguments":["1536*((s1//2))","(s0//2)"],"result":"1536*((s0//2))*((s1//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},{"method":"gt","arguments":["1536*((s0//2))*((s1//2))","1"],"result":"1536*((s0//2))*((s1//2)) > 1","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},{"method":"eq","arguments":["((s0//2))*((s1//2))","-1"],"result":"Eq(((s0//2))*((s1//2)), -1)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 931 in infer_size>"},{"method":"ge","arguments":["((s0//2))*((s1//2))","0"],"result":"((s0//2))*((s1//2)) >= 0","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 934 in infer_size>"},{"method":"mul","arguments":["((s0//2))*((s1//2))","1"],"result":"((s0//2))*((s1//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 935 in infer_size>"},{"method":"mul","arguments":["((s0//2))*((s1//2))","1536"],"result":"1536*((s0//2))*((s1//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 935 in infer_size>"},{"method":"eq","arguments":["1536*((s0//2))*((s1//2))","1536*((s0//2))*((s1//2))"],"result":"True","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 940 in infer_size>"},{"method":"eq","arguments":["1536*((s0//2))*((s1//2))","0"],"result":"Eq(1536*((s0//2))*((s1//2)), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_refs/__init__.py, line 3716 in _reshape_view_helper>"},{"method":"eq","arguments":["((s0//2))*((s1//2))","(s0//2)"],"result":"Eq(((s0//2))*((s1//2)), (s0//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_refs/__init__.py, line 3784 in _reshape_view_helper>"},{"method":"mod","arguments":["(s0//2)","((s0//2))*((s1//2))"],"result":"Mod((s0//2), ((s0//2))*((s1//2)))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_refs/__init__.py, line 3793 in _reshape_view_helper>"},{"method":"ne","arguments":["Mod((s0//2), ((s0//2))*((s1//2)))","0"],"result":"Ne(Mod((s0//2), ((s0//2))*((s1//2))), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_refs/__init__.py, line 3793 in _reshape_view_helper>"},{"method":"mul","arguments":["(s0//2)","(s1//2)"],"result":"((s0//2))*((s1//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_refs/__init__.py, line 3795 in _reshape_view_helper>"},{"method":"mod","arguments":["((s0//2))*((s1//2))","((s0//2))*((s1//2))"],"result":"0","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_refs/__init__.py, line 3793 in _reshape_view_helper>"},{"method":"ne","arguments":["0","0"],"result":"False","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_refs/__init__.py, line 3793 in _reshape_view_helper>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1398 in _collapse_view_helper>"},{"method":"eq","arguments":["(s1//2)","0"],"result":"Eq((s1//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1399 in _collapse_view_helper>"},{"method":"eq","arguments":["(s0//2)","1"],"result":"Eq((s0//2), 1)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1405 in _collapse_view_helper>"},{"method":"mul","arguments":["(s1//2)","(s0//2)"],"result":"((s0//2))*((s1//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1408 in _collapse_view_helper>"},{"method":"gt","arguments":["1536*((s1//2))","1536"],"result":"1536*((s1//2)) > 1536","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1409 in _collapse_view_helper>"},{"method":"gt","arguments":["1536*((s0//2))*((s1//2))","0"],"result":"1536*((s0//2))*((s1//2)) > 0","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1415 in _collapse_view_helper>"},{"method":"ne","arguments":["(s1//2)","1"],"result":"Ne((s1//2), 1)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1416 in _collapse_view_helper>"},{"method":"mul","arguments":["(s1//2)","1536"],"result":"1536*((s1//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},{"method":"eq","arguments":["1536*((s1//2))","1536*((s1//2))"],"result":"True","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},{"method":"eq","arguments":["1536*((s0//2))*((s1//2))","0"],"result":"Eq(1536*((s0//2))*((s1//2)), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1427 in _collapse_view_helper>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1398 in _collapse_view_helper>"},{"method":"eq","arguments":["(s1//2)","0"],"result":"Eq((s1//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1399 in _collapse_view_helper>"},{"method":"eq","arguments":["(s0//2)","1"],"result":"Eq((s0//2), 1)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1405 in _collapse_view_helper>"},{"method":"mul","arguments":["(s1//2)","(s0//2)"],"result":"((s0//2))*((s1//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1408 in _collapse_view_helper>"},{"method":"gt","arguments":["1536*((s1//2))","1536"],"result":"1536*((s1//2)) > 1536","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1409 in _collapse_view_helper>"},{"method":"gt","arguments":["1536*((s0//2))*((s1//2))","0"],"result":"1536*((s0//2))*((s1//2)) > 0","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1415 in _collapse_view_helper>"},{"method":"ne","arguments":["(s1//2)","1"],"result":"Ne((s1//2), 1)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1416 in _collapse_view_helper>"},{"method":"mul","arguments":["(s1//2)","1536"],"result":"1536*((s1//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},{"method":"eq","arguments":["1536*((s1//2))","1536*((s1//2))"],"result":"True","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},{"method":"eq","arguments":["1536*((s0//2))*((s1//2))","0"],"result":"Eq(1536*((s0//2))*((s1//2)), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1427 in _collapse_view_helper>"},{"method":"eq","arguments":["(s0//2)","0"],"result":"Eq((s0//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1398 in _collapse_view_helper>"},{"method":"eq","arguments":["(s1//2)","0"],"result":"Eq((s1//2), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1399 in _collapse_view_helper>"},{"method":"eq","arguments":["(s0//2)","1"],"result":"Eq((s0//2), 1)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1405 in _collapse_view_helper>"},{"method":"mul","arguments":["(s1//2)","(s0//2)"],"result":"((s0//2))*((s1//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1408 in _collapse_view_helper>"},{"method":"gt","arguments":["1536*((s1//2))","1536"],"result":"1536*((s1//2)) > 1536","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1409 in _collapse_view_helper>"},{"method":"gt","arguments":["1536*((s0//2))*((s1//2))","0"],"result":"1536*((s0//2))*((s1//2)) > 0","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1415 in _collapse_view_helper>"},{"method":"ne","arguments":["(s1//2)","1"],"result":"Ne((s1//2), 1)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1416 in _collapse_view_helper>"},{"method":"mul","arguments":["(s1//2)","1536"],"result":"1536*((s1//2))","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},{"method":"eq","arguments":["1536*((s1//2))","1536*((s1//2))"],"result":"True","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},{"method":"eq","arguments":["1536*((s0//2))*((s1//2))","0"],"result":"Eq(1536*((s0//2))*((s1//2)), 0)","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_prims/__init__.py, line 1427 in _collapse_view_helper>"},{"method":"ne","arguments":["((s0//2))*((s1//2))","((s0//2))*((s1//2))"],"result":"False","user_bottom_stack":"spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_refs/__init__.py, line 3813 in _reshape_view_helper>"},{"method":"eq","arguments":["((s0//2))*((s1//2))","1"],"result":"Eq(((s0//2))*((s1//2)), 1)","user_bottom_stack":"return (latent + pos_embed).to(latent.dtype)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:569 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_subclasses/fake_impls.py, line 846 in infer_size>"},{"method":"eq","arguments":["((s0//2))*((s1//2))","1"],"result":"Eq(((s0//2))*((s1//2)), 1)","user_bottom_stack":"return (latent + pos_embed).to(latent.dtype)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:569 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_subclasses/fake_impls.py, line 847 in infer_size>"},{"method":"eq","arguments":["((s0//2))*((s1//2))","((s0//2))*((s1//2))"],"result":"True","user_bottom_stack":"return (latent + pos_embed).to(latent.dtype)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:569 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_subclasses/fake_impls.py, line 848 in infer_size>"},{"method":"eq","arguments":["((s0//2))*((s1//2))","1"],"result":"Eq(((s0//2))*((s1//2)), 1)","user_bottom_stack":"return (latent + pos_embed).to(latent.dtype)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:569 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_subclasses/fake_impls.py, line 853 in infer_size>"},{"method":"eq","arguments":["((s0//2))*((s1//2))","1"],"result":"Eq(((s0//2))*((s1//2)), 1)","user_bottom_stack":"return (latent + pos_embed).to(latent.dtype)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:569 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_subclasses/fake_impls.py, line 846 in infer_size>"},{"method":"eq","arguments":["((s0//2))*((s1//2))","1"],"result":"Eq(((s0//2))*((s1//2)), 1)","user_bottom_stack":"return (latent + pos_embed).to(latent.dtype)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:569 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_subclasses/fake_impls.py, line 847 in infer_size>"},{"method":"eq","arguments":["((s0//2))*((s1//2))","((s0//2))*((s1//2))"],"result":"True","user_bottom_stack":"return (latent + pos_embed).to(latent.dtype)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:569 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_subclasses/fake_impls.py, line 848 in infer_size>"},{"method":"eq","arguments":["((s0//2))*((s1//2))","1"],"result":"Eq(((s0//2))*((s1//2)), 1)","user_bottom_stack":"return (latent + pos_embed).to(latent.dtype)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:569 in forward","user_top_stack":"hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward","floc":"<FrameSummary file pytorch/torch/_subclasses/fake_impls.py, line 853 in infer_size>"}]
