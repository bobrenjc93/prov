import { useEffect, useState } from "react";
import { Button } from "~/components/ui/button";
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from "~/components/ui/dialog";
import { Progress } from "~/components/ui/progress";
import { Textarea } from "~/components/ui/textarea";
import { Accordion, AccordionItem, AccordionTrigger, AccordionContent } from "~/components/ui/accordion";

type ProvenanceEntry = {
  method: string;
  arguments: string[];
  result: string;
  user_bottom_stack: string;
  user_stack: string;
  framework_stack: string;
};

export function meta() {
  return [
    { title: "Expression Provenance Viewer" },
    {
      name: "description",
      content: "Visualize expression provenance and stack traces"
    },
  ];
}

let FLAG = true;

export default function Home() {
  const [provenanceData, setProvenanceData] = useState<ProvenanceEntry[]>(EXAMPLE_DATA);
  const [isLoading, setIsLoading] = useState(false);
  const [progress, setProgress] = useState(0);
  const [dialogOpen, setDialogOpen] = useState(false);
  const [dataInput, setDataInput] = useState("");
  const [searchTerm, setSearchTerm] = useState("");

  console.log(JSON.stringify(provenanceData))

  useEffect(() => {
    const loadData = async () => {
      const urlParams = new URLSearchParams(window.location.search);
      const url = urlParams.get('url');
      if (url) {
        setIsLoading(true);
        try {
          const response = await fetch(url);
          if (response.ok) {
            const jsonData = await response.json();
            setProvenanceData(jsonData);
          }
        } catch (error) {
          console.error("Error loading data from URL:", error);
        } finally {
          setIsLoading(false);
        }
      } else if (FLAG) {
        setDialogOpen(true);
      }
    };
    loadData();
  }, []);

  // Build a map of expressions to their provenance entries
  const buildExpressionMap = (entries: ProvenanceEntry[]) => {    const expressionMap = new Map<string, ProvenanceEntry>();
    const dependencyMap = new Map<string, Set<string>>();
    const argumentMap = new Map<string, Set<string>>();

    entries.forEach(entry => {
      expressionMap.set(entry.result, entry);

      // Track dependencies (results that depend on this result)
      entry.arguments.forEach(arg => {
        if (!dependencyMap.has(arg)) {
          dependencyMap.set(arg, new Set());
        }
        dependencyMap.get(arg)?.add(entry.result);

        // Track arguments (results that this result depends on)
        if (!argumentMap.has(entry.result)) {
          argumentMap.set(entry.result, new Set());
        }
        argumentMap.get(entry.result)?.add(arg);
      });
    });

    return { expressionMap, dependencyMap, argumentMap };
  };

  const renderExpressionTrie = (
    expr: string,
    expressionMap: Map<string, ProvenanceEntry>,
    argumentMap: Map<string, Set<string>>,
    depth = 0,
    visited = new Set<string>()
  ): JSX.Element | null => {
    if (visited.has(expr)) return null;
    visited.add(expr);

    const entry = expressionMap.get(expr);
    if (!entry) return null;

    const args = argumentMap.get(expr) || new Set();
    const childrenElements: JSX.Element[] = [];

    // Only render children that are in the parent's arguments array
    entry.arguments.forEach(parentArg => {
      if (expressionMap.has(parentArg)) {
        const childElement = renderExpressionTrie(parentArg, expressionMap, argumentMap, depth + 1, visited);
        if (childElement) {
          childrenElements.push(childElement);
        }
      }
    });

    return (
      <div key={expr} className="mb-2" style={{ marginLeft: `${depth * 20}px` }}>
        <div className="p-4 border rounded shadow bg-white">
          <h3 className="font-bold text-lg">{expr}</h3>
          <div className="mt-2">
            <p><span className="font-semibold">Method:</span> {entry.method}</p>
            <p><span className="font-semibold">Arguments:</span> {entry.arguments.join(", ")}</p>
            <Accordion type="single" collapsible>
              <AccordionItem value="userStack">
                <AccordionTrigger>User Stack</AccordionTrigger>
                <AccordionContent>
                  <pre>{entry.user_stack || entry.user_top_stack}</pre>
                </AccordionContent>
              </AccordionItem>
              <AccordionItem value="stack">
                <AccordionTrigger>Framework Stack</AccordionTrigger>
                <AccordionContent>
                  <pre>{entry.framework_stack}</pre>
                </AccordionContent>
              </AccordionItem>
            </Accordion>
          </div>
        </div>
        {childrenElements.length > 0 && (
          <div className="ml-4 pl-4 border-l border-gray-300 mt-2">
            {childrenElements}
          </div>
        )}
      </div>
    );
  };

  const { expressionMap, dependencyMap, argumentMap } = buildExpressionMap(provenanceData);

  const handleDataSubmit = () => {
    if (dataInput) {
      let parsedData;
      try {
        // Handle both regular JSON and line-delimited JSON
        const lines = [...new Set(dataInput.split('\n').filter(line => line.trim()))];
        if (lines.length > 1) {
          // Try parsing as line-delimited JSON
          parsedData = lines.flatMap(line => {
            try {
              return [JSON.parse(line)];
            } catch {
              return [];
            }
          });
        } else {
          // Try parsing as regular JSON
          parsedData = JSON.parse(dataInput);
        }

        if (Array.isArray(parsedData)) {
          FLAG = false;
          setProvenanceData(parsedData);
        } else {
          alert("Invalid data format. Using example data instead.");
          setProvenanceData(EXAMPLE_DATA);
        }
      } catch {
        alert("Invalid JSON format. Using example data instead.");
        setProvenanceData(EXAMPLE_DATA);
      }
    } else {
      setProvenanceData(EXAMPLE_DATA);
    }
    setProgress(100);
    setIsLoading(false);
    setDialogOpen(false);
  };

  if (isLoading) {
    return (
      <div className="h-screen flex flex-col gap-4 items-center justify-center">
        <Progress value={progress} className="w-[60%]" />
        <p className="text-sm text-muted-foreground">Loading files...</p>
      </div>
    );
  }

  const filteredExpressions = searchTerm
    ? Array.from(expressionMap.keys()).filter(expr => expr.toLowerCase().includes(searchTerm.toLowerCase()))
    : Array.from(expressionMap.keys());

  return (
    <div className="h-screen flex flex-col">
      <Dialog open={dialogOpen} onOpenChange={setDialogOpen}>
        <DialogContent>
          <DialogHeader>
            <DialogTitle>Custom Provenance Data</DialogTitle>
            <DialogDescription>
              Would you like to provide your own provenance data? If not, an example will be used.
            </DialogDescription>
          </DialogHeader>
          <Textarea
            placeholder="Paste your provenance data as JSON array..."
            value={dataInput}
            onChange={(e) => setDataInput(e.target.value)}
            className="min-h-[400px]"
          />
          <DialogFooter>
            <Button variant="outline" onClick={() => setDialogOpen(false)}>
              Use Example
            </Button>
            <Button onClick={handleDataSubmit}>
              Submit
            </Button>
          </DialogFooter>
        </DialogContent>
      </Dialog>

      <div className="p-4 border-b">
        <input
          type="text"
          placeholder="Search for an expression..."
          value={searchTerm}
          onChange={(e) => setSearchTerm(e.target.value)}
          className="w-full p-2 border rounded"
        />
      </div>

      <div className="flex-1 overflow-auto p-4">
        {filteredExpressions
          .sort((a, b) => {
            // Count operators and operands as a simple complexity measure
            const countComplexity = (expr: string) => {
              return (expr.match(/[+\-*/><=]/g) || []).length +
                     (expr.match(/\w+/g) || []).length;
            };
            return countComplexity(b) - countComplexity(a);
          })
          .map(expr =>
            renderExpressionTrie(expr, expressionMap, argumentMap)
          )}
      </div>
    </div>
  );
}

const EXAMPLE_DATA = [
  {"method": "add", "arguments": ["s0", "0"], "result": "s0", "user_bottom_stack": "latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_meta_registrations.py, line 2183 in _formula>"},
  {"method": "sub", "arguments": ["s0", "1"], "result": "s0 - 1", "user_bottom_stack": "latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_meta_registrations.py, line 2183 in _formula>"},
  {"method": "sub", "arguments": ["s0 - 1", "1"], "result": "s0 - 2", "user_bottom_stack": "latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_meta_registrations.py, line 2183 in _formula>"},
  {"method": "int_floordiv", "arguments": ["s0 - 2", "2"], "result": "((s0//2)) - 1", "user_bottom_stack": "latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_meta_registrations.py, line 2183 in _formula>"},
  {"method": "add", "arguments": ["((s0//2)) - 1", "1"], "result": "(s0//2)", "user_bottom_stack": "latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_meta_registrations.py, line 2183 in _formula>"},
  {"method": "add", "arguments": ["s1", "0"], "result": "s1", "user_bottom_stack": "latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_meta_registrations.py, line 2183 in _formula>"},
  {"method": "sub", "arguments": ["s1", "1"], "result": "s1 - 1", "user_bottom_stack": "latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_meta_registrations.py, line 2183 in _formula>"},
  {"method": "sub", "arguments": ["s1 - 1", "1"], "result": "s1 - 2", "user_bottom_stack": "latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_meta_registrations.py, line 2183 in _formula>"},
  {"method": "int_floordiv", "arguments": ["s1 - 2", "2"], "result": "((s1//2)) - 1", "user_bottom_stack": "latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_meta_registrations.py, line 2183 in _formula>"},
  {"method": "add", "arguments": ["((s1//2)) - 1", "1"], "result": "(s1//2)", "user_bottom_stack": "latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_meta_registrations.py, line 2183 in _formula>"},
  {"method": "gt", "arguments": ["(s0//2)", "0"], "result": "((s0//2)) > 0", "user_bottom_stack": "latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_meta_registrations.py, line 2256 in <genexpr>>"},
  {"method": "lt", "arguments": ["(s1//2)", "1"], "result": "((s1//2)) < 1", "user_bottom_stack": "latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_refs/__init__.py, line 4800 in new_empty>"},
  {"method": "lt", "arguments": ["(s0//2)", "1"], "result": "((s0//2)) < 1", "user_bottom_stack": "latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_refs/__init__.py, line 4800 in new_empty>"},
  {"method": "eq", "arguments": ["s0*s1", "0"], "result": "False", "user_bottom_stack": "latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1883 in are_strides_like_channels_last>"},
  {"method": "lt", "arguments": ["s0*s1", "0"], "result": "False", "user_bottom_stack": "latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},
  {"method": "gt", "arguments": ["s0*s1", "1"], "result": "s0*s1 > 1", "user_bottom_stack": "latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1895 in are_strides_like_channels_last>"},
  {"method": "mul", "arguments": ["s0*s1", "16"], "result": "16*s0*s1", "user_bottom_stack": "latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},
  {"method": "eq", "arguments": ["s1", "0"], "result": "False", "user_bottom_stack": "latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},
  {"method": "gt", "arguments": ["16*s0*s1", "1"], "result": "True", "user_bottom_stack": "latent = self.proj(latent)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:545 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},
  {"method": "eq", "arguments": ["((s0//2))*((s1//2))", "-1"], "result": "Eq(((s0//2))*((s1//2)), -1)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 931 in infer_size>"},
  {"method": "ge", "arguments": ["((s0//2))*((s1//2))", "0"], "result": "((s0//2))*((s1//2)) >= 0", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 934 in infer_size>"},
  {"method": "mul", "arguments": ["((s0//2))*((s1//2))", "3072"], "result": "3072*((s0//2))*((s1//2))", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 935 in infer_size>"},
  {"method": "eq", "arguments": ["3072*((s0//2))*((s1//2))", "3072*((s0//2))*((s1//2))"], "result": "True", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 940 in infer_size>"},
  {"method": "eq", "arguments": ["3072*((s0//2))*((s1//2))", "0"], "result": "Eq(3072*((s0//2))*((s1//2)), 0)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_refs/__init__.py, line 3716 in _reshape_view_helper>"},
  {"method": "eq", "arguments": ["((s0//2))*((s1//2))", "(s0//2)"], "result": "Eq(((s0//2))*((s1//2)), (s0//2))", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_refs/__init__.py, line 3784 in _reshape_view_helper>"},
  {"method": "mod", "arguments": ["(s0//2)", "((s0//2))*((s1//2))"], "result": "Mod((s0//2), ((s0//2))*((s1//2)))", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_refs/__init__.py, line 3793 in _reshape_view_helper>"},
  {"method": "ne", "arguments": ["Mod((s0//2), ((s0//2))*((s1//2)))", "0"], "result": "Ne(Mod((s0//2), ((s0//2))*((s1//2))), 0)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_refs/__init__.py, line 3793 in _reshape_view_helper>"},
  {"method": "mul", "arguments": ["(s0//2)", "(s1//2)"], "result": "((s0//2))*((s1//2))", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_refs/__init__.py, line 3795 in _reshape_view_helper>"},
  {"method": "mod", "arguments": ["((s0//2))*((s1//2))", "((s0//2))*((s1//2))"], "result": "0", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_refs/__init__.py, line 3793 in _reshape_view_helper>"},
  {"method": "ne", "arguments": ["0", "0"], "result": "False", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_refs/__init__.py, line 3793 in _reshape_view_helper>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1398 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["(s1//2)", "0"], "result": "Eq((s1//2), 0)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1399 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["(s0//2)", "1"], "result": "Eq((s0//2), 1)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1405 in _collapse_view_helper>"},
  {"method": "mul", "arguments": ["(s1//2)", "(s0//2)"], "result": "((s0//2))*((s1//2))", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1408 in _collapse_view_helper>"},
  {"method": "gt", "arguments": ["(s1//2)", "1"], "result": "((s1//2)) > 1", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1409 in _collapse_view_helper>"},
  {"method": "gt", "arguments": ["3072*((s0//2))*((s1//2))", "0"], "result": "3072*((s0//2))*((s1//2)) > 0", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1415 in _collapse_view_helper>"},
  {"method": "ne", "arguments": ["(s1//2)", "1"], "result": "Ne((s1//2), 1)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1416 in _collapse_view_helper>"},
  {"method": "mul", "arguments": ["(s1//2)", "1"], "result": "(s1//2)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["(s1//2)", "(s1//2)"], "result": "True", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["3072*((s0//2))*((s1//2))", "0"], "result": "Eq(3072*((s0//2))*((s1//2)), 0)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1427 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1398 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["(s1//2)", "0"], "result": "Eq((s1//2), 0)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1399 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["(s0//2)", "1"], "result": "Eq((s0//2), 1)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1405 in _collapse_view_helper>"},
  {"method": "mul", "arguments": ["(s1//2)", "(s0//2)"], "result": "((s0//2))*((s1//2))", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1408 in _collapse_view_helper>"},
  {"method": "gt", "arguments": ["(s1//2)", "1"], "result": "((s1//2)) > 1", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1409 in _collapse_view_helper>"},
  {"method": "gt", "arguments": ["3072*((s0//2))*((s1//2))", "0"], "result": "3072*((s0//2))*((s1//2)) > 0", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1415 in _collapse_view_helper>"},
  {"method": "ne", "arguments": ["(s1//2)", "1"], "result": "Ne((s1//2), 1)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1416 in _collapse_view_helper>"},
  {"method": "mul", "arguments": ["(s1//2)", "1"], "result": "(s1//2)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["(s1//2)", "(s1//2)"], "result": "True", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["3072*((s0//2))*((s1//2))", "0"], "result": "Eq(3072*((s0//2))*((s1//2)), 0)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1427 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1398 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["(s1//2)", "0"], "result": "Eq((s1//2), 0)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1399 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["(s0//2)", "1"], "result": "Eq((s0//2), 1)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1405 in _collapse_view_helper>"},
  {"method": "mul", "arguments": ["(s1//2)", "(s0//2)"], "result": "((s0//2))*((s1//2))", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1408 in _collapse_view_helper>"},
  {"method": "gt", "arguments": ["(s1//2)", "1"], "result": "((s1//2)) > 1", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1409 in _collapse_view_helper>"},
  {"method": "gt", "arguments": ["3072*((s0//2))*((s1//2))", "0"], "result": "3072*((s0//2))*((s1//2)) > 0", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1415 in _collapse_view_helper>"},
  {"method": "ne", "arguments": ["(s1//2)", "1"], "result": "Ne((s1//2), 1)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1416 in _collapse_view_helper>"},
  {"method": "mul", "arguments": ["(s1//2)", "1"], "result": "(s1//2)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["(s1//2)", "(s1//2)"], "result": "True", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["3072*((s0//2))*((s1//2))", "0"], "result": "Eq(3072*((s0//2))*((s1//2)), 0)", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1427 in _collapse_view_helper>"},
  {"method": "ne", "arguments": ["((s0//2))*((s1//2))", "((s0//2))*((s1//2))"], "result": "False", "user_bottom_stack": "latent = latent.flatten(2).transpose(1, 2)  # BCHW -> BNC  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:547 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_refs/__init__.py, line 3813 in _reshape_view_helper>"},
  {"method": "int_floordiv", "arguments": ["s0", "2"], "result": "(s0//2)", "user_bottom_stack": "height = height // self.patch_size  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:522 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_dynamo/variables/builder.py, line 2322 in _wrap_fx_proxy>"},
  {"method": "int_floordiv", "arguments": ["s1", "2"], "result": "(s1//2)", "user_bottom_stack": "width = width // self.patch_size  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:523 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_dynamo/variables/builder.py, line 2322 in _wrap_fx_proxy>"},
  {"method": "gt", "arguments": ["(s0//2)", "192"], "result": "((s0//2)) > 192", "user_bottom_stack": "if height > self.pos_embed_max_size:  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:524 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_dynamo/variables/tensor.py, line 1164 in create>"},
  {"method": "gt", "arguments": ["(s1//2)", "192"], "result": "((s1//2)) > 192", "user_bottom_stack": "if width > self.pos_embed_max_size:  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:528 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_dynamo/variables/tensor.py, line 1164 in create>"},
  {"method": "sub", "arguments": ["(s0//2)", "192"], "result": "192 - ((s0//2))", "user_bottom_stack": "top = (self.pos_embed_max_size - height) // 2  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:533 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_dynamo/variables/builder.py, line 2322 in _wrap_fx_proxy>"},
  {"method": "int_floordiv", "arguments": ["192 - ((s0//2))", "2"], "result": "(((-((s0//2)))//2)) + 96", "user_bottom_stack": "top = (self.pos_embed_max_size - height) // 2  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:533 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_dynamo/variables/builder.py, line 2322 in _wrap_fx_proxy>"},
  {"method": "sub", "arguments": ["(s1//2)", "192"], "result": "192 - ((s1//2))", "user_bottom_stack": "left = (self.pos_embed_max_size - width) // 2  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:534 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_dynamo/variables/builder.py, line 2322 in _wrap_fx_proxy>"},
  {"method": "int_floordiv", "arguments": ["192 - ((s1//2))", "2"], "result": "(((-((s1//2)))//2)) + 96", "user_bottom_stack": "left = (self.pos_embed_max_size - width) // 2  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:534 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_dynamo/variables/builder.py, line 2322 in _wrap_fx_proxy>"},
  {"method": "add", "arguments": ["(((-((s0//2)))//2)) + 96", "(s0//2)"], "result": "((s0//2)) + (((-((s0//2)))//2)) + 96", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_dynamo/variables/builder.py, line 2322 in _wrap_fx_proxy>"},
  {"method": "add", "arguments": ["(((-((s1//2)))//2)) + 96", "(s1//2)"], "result": "((s1//2)) + (((-((s1//2)))//2)) + 96", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_dynamo/variables/builder.py, line 2322 in _wrap_fx_proxy>"},
  {"method": "lt", "arguments": ["(((-((s0//2)))//2)) + 96", "0"], "result": "(((-((s0//2)))//2)) + 96 < 0", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 730 in slice_forward>"},
  {"method": "lt", "arguments": ["((s0//2)) + (((-((s0//2)))//2)) + 96", "0"], "result": "((s0//2)) + (((-((s0//2)))//2)) + 96 < 0", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 733 in slice_forward>"},
  {"method": "lt", "arguments": ["(((-((s0//2)))//2)) + 96", "0"], "result": "(((-((s0//2)))//2)) + 96 < 0", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 736 in slice_forward>"},
  {"method": "gt", "arguments": ["(((-((s0//2)))//2)) + 96", "192"], "result": "(((-((s0//2)))//2)) + 96 > 192", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 738 in slice_forward>"},
  {"method": "lt", "arguments": ["((s0//2)) + (((-((s0//2)))//2)) + 96", "(((-((s0//2)))//2)) + 96"], "result": "((s0//2)) + (((-((s0//2)))//2)) + 96 < (((-((s0//2)))//2)) + 96", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 741 in slice_forward>"},
  {"method": "eq", "arguments": ["((s0//2)) + (((-((s0//2)))//2)) + 96", "9223372036854775807"], "result": "Eq(((s0//2)) + (((-((s0//2)))//2)) + 96, 9223372036854775807)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 743 in slice_forward>"},
  {"method": "gt", "arguments": ["((s0//2)) + (((-((s0//2)))//2)) + 96", "192"], "result": "((s0//2)) + (((-((s0//2)))//2)) + 96 > 192", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 744 in slice_forward>"},
  {"method": "mul", "arguments": ["(((-((s0//2)))//2)) + 96", "294912"], "result": "294912*(((-((s0//2)))//2)) + 28311552", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 748 in slice_forward>"},
  {"method": "add", "arguments": ["294912*(((-((s0//2)))//2)) + 28311552", "0"], "result": "294912*(((-((s0//2)))//2)) + 28311552", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 748 in slice_forward>"},
  {"method": "sub", "arguments": ["((s0//2)) + (((-((s0//2)))//2)) + 96", "(((-((s0//2)))//2)) + 96"], "result": "(s0//2)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 749 in slice_forward>"},
  {"method": "add", "arguments": ["(s0//2)", "1"], "result": "((s0//2)) + 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 750 in slice_forward>"},
  {"method": "sub", "arguments": ["((s0//2)) + 1", "1"], "result": "(s0//2)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 750 in slice_forward>"},
  {"method": "int_floordiv", "arguments": ["(s0//2)", "1"], "result": "(s0//2)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 750 in slice_forward>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},
  {"method": "mul", "arguments": ["(s0//2)", "294912"], "result": "294912*((s0//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},
  {"method": "gt", "arguments": ["294912*((s0//2))", "1"], "result": "294912*((s0//2)) > 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},
  {"method": "mul", "arguments": ["(s0//2)", "294912"], "result": "294912*((s0//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},
  {"method": "gt", "arguments": ["294912*((s0//2))", "1"], "result": "294912*((s0//2)) > 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},
  {"method": "mul", "arguments": ["(s0//2)", "294912"], "result": "294912*((s0//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},
  {"method": "gt", "arguments": ["294912*((s0//2))", "1"], "result": "294912*((s0//2)) > 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},
  {"method": "lt", "arguments": ["(((-((s1//2)))//2)) + 96", "0"], "result": "(((-((s1//2)))//2)) + 96 < 0", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 730 in slice_forward>"},
  {"method": "lt", "arguments": ["((s1//2)) + (((-((s1//2)))//2)) + 96", "0"], "result": "((s1//2)) + (((-((s1//2)))//2)) + 96 < 0", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 733 in slice_forward>"},
  {"method": "lt", "arguments": ["(((-((s1//2)))//2)) + 96", "0"], "result": "(((-((s1//2)))//2)) + 96 < 0", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 736 in slice_forward>"},
  {"method": "gt", "arguments": ["(((-((s1//2)))//2)) + 96", "192"], "result": "(((-((s1//2)))//2)) + 96 > 192", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 738 in slice_forward>"},
  {"method": "lt", "arguments": ["((s1//2)) + (((-((s1//2)))//2)) + 96", "(((-((s1//2)))//2)) + 96"], "result": "((s1//2)) + (((-((s1//2)))//2)) + 96 < (((-((s1//2)))//2)) + 96", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 741 in slice_forward>"},
  {"method": "eq", "arguments": ["((s1//2)) + (((-((s1//2)))//2)) + 96", "9223372036854775807"], "result": "Eq(((s1//2)) + (((-((s1//2)))//2)) + 96, 9223372036854775807)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 743 in slice_forward>"},
  {"method": "gt", "arguments": ["((s1//2)) + (((-((s1//2)))//2)) + 96", "192"], "result": "((s1//2)) + (((-((s1//2)))//2)) + 96 > 192", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 744 in slice_forward>"},
  {"method": "mul", "arguments": ["(((-((s1//2)))//2)) + 96", "1536"], "result": "1536*(((-((s1//2)))//2)) + 147456", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 748 in slice_forward>"},
  {"method": "add", "arguments": ["294912*(((-((s0//2)))//2)) + 28311552", "1536*(((-((s1//2)))//2)) + 147456"], "result": "294912*(((-((s0//2)))//2)) + 1536*(((-((s1//2)))//2)) + 28459008", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 748 in slice_forward>"},
  {"method": "sub", "arguments": ["((s1//2)) + (((-((s1//2)))//2)) + 96", "(((-((s1//2)))//2)) + 96"], "result": "(s1//2)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 749 in slice_forward>"},
  {"method": "add", "arguments": ["(s1//2)", "1"], "result": "((s1//2)) + 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 750 in slice_forward>"},
  {"method": "sub", "arguments": ["((s1//2)) + 1", "1"], "result": "(s1//2)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 750 in slice_forward>"},
  {"method": "int_floordiv", "arguments": ["(s1//2)", "1"], "result": "(s1//2)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 750 in slice_forward>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},
  {"method": "mul", "arguments": ["(s0//2)", "294912"], "result": "294912*((s0//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},
  {"method": "gt", "arguments": ["294912*((s0//2))", "1"], "result": "294912*((s0//2)) > 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},
  {"method": "mul", "arguments": ["(s0//2)", "294912"], "result": "294912*((s0//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},
  {"method": "gt", "arguments": ["294912*((s0//2))", "1"], "result": "294912*((s0//2)) > 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},
  {"method": "mul", "arguments": ["(s0//2)", "294912"], "result": "294912*((s0//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},
  {"method": "gt", "arguments": ["294912*((s0//2))", "1"], "result": "294912*((s0//2)) > 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},
  {"method": "mul", "arguments": ["(s0//2)", "294912"], "result": "294912*((s0//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},
  {"method": "gt", "arguments": ["294912*((s0//2))", "1"], "result": "294912*((s0//2)) > 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},
  {"method": "add", "arguments": ["294912*(((-((s0//2)))//2)) + 1536*(((-((s1//2)))//2)) + 28459008", "0"], "result": "294912*(((-((s0//2)))//2)) + 1536*(((-((s1//2)))//2)) + 28459008", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_decomp/decompositions.py, line 748 in slice_forward>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},
  {"method": "mul", "arguments": ["(s0//2)", "294912"], "result": "294912*((s0//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},
  {"method": "gt", "arguments": ["294912*((s0//2))", "1"], "result": "294912*((s0//2)) > 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},
  {"method": "mul", "arguments": ["(s0//2)", "294912"], "result": "294912*((s0//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},
  {"method": "gt", "arguments": ["294912*((s0//2))", "1"], "result": "294912*((s0//2)) > 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},
  {"method": "mul", "arguments": ["(s0//2)", "294912"], "result": "294912*((s0//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},
  {"method": "gt", "arguments": ["294912*((s0//2))", "1"], "result": "294912*((s0//2)) > 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed[:, top : top + height, left : left + width, :]  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:536 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},
  {"method": "mul", "arguments": ["(s0//2)", "294912"], "result": "294912*((s0//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},
  {"method": "gt", "arguments": ["294912*((s0//2))", "1"], "result": "294912*((s0//2)) > 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},
  {"method": "mul", "arguments": ["(s0//2)", "294912"], "result": "294912*((s0//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},
  {"method": "gt", "arguments": ["294912*((s0//2))", "1"], "result": "294912*((s0//2)) > 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},
  {"method": "lt", "arguments": ["(s1//2)", "1"], "result": "((s1//2)) < 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_ops.py, line 758 in __call__>"},
  {"method": "lt", "arguments": ["(s0//2)", "1"], "result": "((s0//2)) < 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_ops.py, line 758 in __call__>"},
  {"method": "eq", "arguments": ["1536*((s1//2))", "0"], "result": "Eq(1536*((s1//2)), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1883 in are_strides_like_channels_last>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},
  {"method": "lt", "arguments": ["1536*((s1//2))", "0"], "result": "1536*((s1//2)) < 0", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},
  {"method": "gt", "arguments": ["1536*((s1//2))", "1"], "result": "1536*((s1//2)) > 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1895 in are_strides_like_channels_last>"},
  {"method": "mul", "arguments": ["1536*((s1//2))", "(s0//2)"], "result": "1536*((s0//2))*((s1//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},
  {"method": "gt", "arguments": ["1536*((s0//2))*((s1//2))", "1"], "result": "1536*((s0//2))*((s1//2)) > 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},
  {"method": "eq", "arguments": ["1536*((s1//2))", "0"], "result": "Eq(1536*((s1//2)), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1883 in are_strides_like_channels_last>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},
  {"method": "lt", "arguments": ["1536*((s1//2))", "0"], "result": "1536*((s1//2)) < 0", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},
  {"method": "gt", "arguments": ["1536*((s1//2))", "1"], "result": "1536*((s1//2)) > 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1895 in are_strides_like_channels_last>"},
  {"method": "mul", "arguments": ["1536*((s1//2))", "(s0//2)"], "result": "1536*((s0//2))*((s1//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},
  {"method": "gt", "arguments": ["1536*((s0//2))*((s1//2))", "1"], "result": "1536*((s0//2))*((s1//2)) > 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},
  {"method": "eq", "arguments": ["1536*((s1//2))", "0"], "result": "Eq(1536*((s1//2)), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1883 in are_strides_like_channels_last>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1888 in are_strides_like_channels_last>"},
  {"method": "lt", "arguments": ["1536*((s1//2))", "0"], "result": "1536*((s1//2)) < 0", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},
  {"method": "gt", "arguments": ["1536*((s1//2))", "1"], "result": "1536*((s1//2)) > 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1895 in are_strides_like_channels_last>"},
  {"method": "mul", "arguments": ["1536*((s1//2))", "(s0//2)"], "result": "1536*((s0//2))*((s1//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1896 in are_strides_like_channels_last>"},
  {"method": "gt", "arguments": ["1536*((s0//2))*((s1//2))", "1"], "result": "1536*((s0//2))*((s1//2)) > 1", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 1890 in are_strides_like_channels_last>"},
  {"method": "eq", "arguments": ["((s0//2))*((s1//2))", "-1"], "result": "Eq(((s0//2))*((s1//2)), -1)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 931 in infer_size>"},
  {"method": "ge", "arguments": ["((s0//2))*((s1//2))", "0"], "result": "((s0//2))*((s1//2)) >= 0", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 934 in infer_size>"},
  {"method": "mul", "arguments": ["((s0//2))*((s1//2))", "1"], "result": "((s0//2))*((s1//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 935 in infer_size>"},
  {"method": "mul", "arguments": ["((s0//2))*((s1//2))", "1536"], "result": "1536*((s0//2))*((s1//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 935 in infer_size>"},
  {"method": "eq", "arguments": ["1536*((s0//2))*((s1//2))", "1536*((s0//2))*((s1//2))"], "result": "True", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims_common/__init__.py, line 940 in infer_size>"},
  {"method": "eq", "arguments": ["1536*((s0//2))*((s1//2))", "0"], "result": "Eq(1536*((s0//2))*((s1//2)), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_refs/__init__.py, line 3716 in _reshape_view_helper>"},
  {"method": "eq", "arguments": ["((s0//2))*((s1//2))", "(s0//2)"], "result": "Eq(((s0//2))*((s1//2)), (s0//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_refs/__init__.py, line 3784 in _reshape_view_helper>"},
  {"method": "mod", "arguments": ["(s0//2)", "((s0//2))*((s1//2))"], "result": "Mod((s0//2), ((s0//2))*((s1//2)))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_refs/__init__.py, line 3793 in _reshape_view_helper>"},
  {"method": "ne", "arguments": ["Mod((s0//2), ((s0//2))*((s1//2)))", "0"], "result": "Ne(Mod((s0//2), ((s0//2))*((s1//2))), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_refs/__init__.py, line 3793 in _reshape_view_helper>"},
  {"method": "mul", "arguments": ["(s0//2)", "(s1//2)"], "result": "((s0//2))*((s1//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_refs/__init__.py, line 3795 in _reshape_view_helper>"},
  {"method": "mod", "arguments": ["((s0//2))*((s1//2))", "((s0//2))*((s1//2))"], "result": "0", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_refs/__init__.py, line 3793 in _reshape_view_helper>"},
  {"method": "ne", "arguments": ["0", "0"], "result": "False", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_refs/__init__.py, line 3793 in _reshape_view_helper>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1398 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["(s1//2)", "0"], "result": "Eq((s1//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1399 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["(s0//2)", "1"], "result": "Eq((s0//2), 1)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1405 in _collapse_view_helper>"},
  {"method": "mul", "arguments": ["(s1//2)", "(s0//2)"], "result": "((s0//2))*((s1//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1408 in _collapse_view_helper>"},
  {"method": "gt", "arguments": ["1536*((s1//2))", "1536"], "result": "1536*((s1//2)) > 1536", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1409 in _collapse_view_helper>"},
  {"method": "gt", "arguments": ["1536*((s0//2))*((s1//2))", "0"], "result": "1536*((s0//2))*((s1//2)) > 0", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1415 in _collapse_view_helper>"},
  {"method": "ne", "arguments": ["(s1//2)", "1"], "result": "Ne((s1//2), 1)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1416 in _collapse_view_helper>"},
  {"method": "mul", "arguments": ["(s1//2)", "1536"], "result": "1536*((s1//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["1536*((s1//2))", "1536*((s1//2))"], "result": "True", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["1536*((s0//2))*((s1//2))", "0"], "result": "Eq(1536*((s0//2))*((s1//2)), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1427 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1398 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["(s1//2)", "0"], "result": "Eq((s1//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1399 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["(s0//2)", "1"], "result": "Eq((s0//2), 1)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1405 in _collapse_view_helper>"},
  {"method": "mul", "arguments": ["(s1//2)", "(s0//2)"], "result": "((s0//2))*((s1//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1408 in _collapse_view_helper>"},
  {"method": "gt", "arguments": ["1536*((s1//2))", "1536"], "result": "1536*((s1//2)) > 1536", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1409 in _collapse_view_helper>"},
  {"method": "gt", "arguments": ["1536*((s0//2))*((s1//2))", "0"], "result": "1536*((s0//2))*((s1//2)) > 0", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1415 in _collapse_view_helper>"},
  {"method": "ne", "arguments": ["(s1//2)", "1"], "result": "Ne((s1//2), 1)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1416 in _collapse_view_helper>"},
  {"method": "mul", "arguments": ["(s1//2)", "1536"], "result": "1536*((s1//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["1536*((s1//2))", "1536*((s1//2))"], "result": "True", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["1536*((s0//2))*((s1//2))", "0"], "result": "Eq(1536*((s0//2))*((s1//2)), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1427 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["(s0//2)", "0"], "result": "Eq((s0//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1398 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["(s1//2)", "0"], "result": "Eq((s1//2), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1399 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["(s0//2)", "1"], "result": "Eq((s0//2), 1)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1405 in _collapse_view_helper>"},
  {"method": "mul", "arguments": ["(s1//2)", "(s0//2)"], "result": "((s0//2))*((s1//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1408 in _collapse_view_helper>"},
  {"method": "gt", "arguments": ["1536*((s1//2))", "1536"], "result": "1536*((s1//2)) > 1536", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1409 in _collapse_view_helper>"},
  {"method": "gt", "arguments": ["1536*((s0//2))*((s1//2))", "0"], "result": "1536*((s0//2))*((s1//2)) > 0", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1415 in _collapse_view_helper>"},
  {"method": "ne", "arguments": ["(s1//2)", "1"], "result": "Ne((s1//2), 1)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1416 in _collapse_view_helper>"},
  {"method": "mul", "arguments": ["(s1//2)", "1536"], "result": "1536*((s1//2))", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["1536*((s1//2))", "1536*((s1//2))"], "result": "True", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1418 in _collapse_view_helper>"},
  {"method": "eq", "arguments": ["1536*((s0//2))*((s1//2))", "0"], "result": "Eq(1536*((s0//2))*((s1//2)), 0)", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_prims/__init__.py, line 1427 in _collapse_view_helper>"},
  {"method": "ne", "arguments": ["((s0//2))*((s1//2))", "((s0//2))*((s1//2))"], "result": "False", "user_bottom_stack": "spatial_pos_embed = spatial_pos_embed.reshape(1, -1, spatial_pos_embed.shape[-1])  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:537 in cropped_pos_embed", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_refs/__init__.py, line 3813 in _reshape_view_helper>"},
  {"method": "eq", "arguments": ["((s0//2))*((s1//2))", "1"], "result": "Eq(((s0//2))*((s1//2)), 1)", "user_bottom_stack": "return (latent + pos_embed).to(latent.dtype)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:569 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_subclasses/fake_impls.py, line 846 in infer_size>"},
  {"method": "eq", "arguments": ["((s0//2))*((s1//2))", "1"], "result": "Eq(((s0//2))*((s1//2)), 1)", "user_bottom_stack": "return (latent + pos_embed).to(latent.dtype)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:569 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_subclasses/fake_impls.py, line 847 in infer_size>"},
  {"method": "eq", "arguments": ["((s0//2))*((s1//2))", "((s0//2))*((s1//2))"], "result": "True", "user_bottom_stack": "return (latent + pos_embed).to(latent.dtype)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:569 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_subclasses/fake_impls.py, line 848 in infer_size>"},
  {"method": "eq", "arguments": ["((s0//2))*((s1//2))", "1"], "result": "Eq(((s0//2))*((s1//2)), 1)", "user_bottom_stack": "return (latent + pos_embed).to(latent.dtype)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:569 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_subclasses/fake_impls.py, line 853 in infer_size>"},
  {"method": "eq", "arguments": ["((s0//2))*((s1//2))", "1"], "result": "Eq(((s0//2))*((s1//2)), 1)", "user_bottom_stack": "return (latent + pos_embed).to(latent.dtype)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:569 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_subclasses/fake_impls.py, line 846 in infer_size>"},
  {"method": "eq", "arguments": ["((s0//2))*((s1//2))", "1"], "result": "Eq(((s0//2))*((s1//2)), 1)", "user_bottom_stack": "return (latent + pos_embed).to(latent.dtype)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:569 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_subclasses/fake_impls.py, line 847 in infer_size>"},
  {"method": "eq", "arguments": ["((s0//2))*((s1//2))", "((s0//2))*((s1//2))"], "result": "True", "user_bottom_stack": "return (latent + pos_embed).to(latent.dtype)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:569 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_subclasses/fake_impls.py, line 848 in infer_size>"},
  {"method": "eq", "arguments": ["((s0//2))*((s1//2))", "1"], "result": "Eq(((s0//2))*((s1//2)), 1)", "user_bottom_stack": "return (latent + pos_embed).to(latent.dtype)  # pytorch-env/lib/python3.10/site-packages/diffusers/models/embeddings.py:569 in forward", "user_stack": "hidden_states = self.pos_embed(hidden_states)  # takes care of adding positional embeddings too.  # pytorch-env/lib/python3.10/site-packages/diffusers/models/transformers/transformer_sd3.py:391 in forward", "framework_stack": "<FrameSummary file pytorch/torch/_subclasses/fake_impls.py, line 853 in infer_size>"},
];
